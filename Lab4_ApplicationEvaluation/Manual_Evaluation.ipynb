{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate application using manual data set\n",
    "\n",
    "## Objective\n",
    "\n",
    "This lab provides a step-by-step guide on how to application endpoints deployed using manual data set\n",
    "\n",
    "Documentation about evaluation SDK - [azure-ai-evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk)\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation\n",
    "%pip install promptflow-azure\n",
    "%pip install azure-identity\n",
    "%pip install --upgrade openai\n",
    "%pip install marshmallow\n",
    "%pip install python-dotenv\n",
    "%pip install azure-ai-evaluation[remote]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by load the configuration from .env file created from the previous step. We also print out the config value for validation. \n",
    "For simplicity, we use key based authentication however Azure AI SDK also support managed indentity. \n",
    "If you hasnt create one please check the [README](README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This notebook uses the shared `.credentials.env` file from the parent directory. \n",
    "\n",
    "For Lab4, you'll also need to add these variables to `.credentials.env`:\n",
    "```\n",
    "APPLICATION_ENDPOINT=\"<your-application-endpoint>\"\n",
    "APPLICATION_KEY=\"<your-application-key>\"\n",
    "```\n",
    "\n",
    "These will be provided by your instructor or team channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "\n",
    "# Load from parent directory's .credentials.env file\n",
    "load_dotenv(\"../.credentials.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Environment variables loaded successfully.\")\n",
    "# These use the same variable names as the other labs\n",
    "print(f\"API Version: {os.environ.get('AZURE_OPENAI_API_VERSION', 'Not set')}\")\n",
    "print(f\"Deployment: {os.environ.get('AZURE_OPENAI_DEPLOYMENT', 'Not set')}\")\n",
    "print(f\"Endpoint: {os.environ.get('AZURE_OPENAI_ENDPOINT', 'Not set')}\")\n",
    "print(f\"Has API Key: {bool(os.environ.get('AZURE_OPENAI_KEY'))}\")\n",
    "print(f\"Resource Group: {os.environ.get('AZURE_RESOURCE_GROUP', 'Not set')}\")\n",
    "print(f\"Project Name: {os.environ.get('AZURE_PROJECT_NAME', 'Not set')}\")\n",
    "print(f\"Application Endpoint: {os.environ.get('APPLICATION_ENDPOINT', 'Not set - you may need to add this')}\")\n",
    "print(f\"Has Application Key: {bool(os.environ.get('APPLICATION_KEY'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Application\n",
    "\n",
    "We will use Evaluate API provided by Azure AI Evaluation SDK. It requires a target endpoint or python Function, which handles a call the application endpoint or a LLM inference endpoint.\n",
    "In this lab we use [application_endpoint.py](application_endpoint.py) to call to a application API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Following code reads Json file \"manual_data.jsonl\" which contains inputs to the application endpoint function. It provides question, context and ground truth on each line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"manual_data.jsonl\", lines=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "To use AI Assisted Evaluator, we will an LLM model details as a Judge that can be passed as model config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    # Use AZURE_OPENAI_API_KEY from .credentials.env (Lab4 README calls it AZURE_OPENAI_KEY)\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\") or os.environ.get(\"AZURE_OPENAI_KEY\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the output, we need to provide Azure AI Project details so that traces and eval results are pushing in the project in Azure AI Studio. NOTE: This is not compulsory to use Azure AI Evaluation SDK. AI Evaluation SDK output the evaluation result so that can be use in CICD pipeline like traditional unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "    \"resource_group_name\": os.environ[\"AZURE_RESOURCE_GROUP\"],  # Using same variable as other labs\n",
    "    \"project_name\": os.environ[\"AZURE_PROJECT_NAME\"],  # Using same variable as other labs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evaluation\n",
    "\n",
    "The Following code runs Evaluate API and uses Content Safety and other metric such as Groundedness to evaluate results from different models.\n",
    "\n",
    "The following are the few parameters required by Evaluate API. \n",
    "\n",
    "+   Data file: It represents data file 'manual_data.jsonl' in JSON format. Each line contains question, context and ground truth for evaluators.     \n",
    "\n",
    "+   Application Target: It is name of python class which can route the calls to specific application endpoints \n",
    "\n",
    "+   Evaluators: List of evaluators is provided, to evaluate given prompts (questions) as input and output (answers) from LLM models. \n",
    "\n",
    "NOTE: If you have error about access storage account please enable key access for your storage account.\n",
    "<details>\n",
    "    <img width=\"500px\" height=\"500px\" src=\"storageconfiguration.png\" alt=\"Storage Account Configuration\" />\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PF_LOGGING_LEVEL'] = 'DEBUG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which endpoint mode is being used\n",
    "# Look at the evaluation cell to see which import is active\n",
    "print(\"üìã Endpoint Configuration Check:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if real endpoint is configured\n",
    "has_real_endpoint = os.environ.get(\"APPLICATION_ENDPOINT\") and os.environ.get(\"APPLICATION_KEY\")\n",
    "\n",
    "if has_real_endpoint:\n",
    "    print(\"‚úÖ Real Application Endpoint Configured\")\n",
    "    print(f\"   Endpoint: {os.environ['APPLICATION_ENDPOINT']}\")\n",
    "    print(\"\\nüí° Make sure to use: from application_endpoint import ApplicationEndpoint\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Real endpoint not configured - using MOCK endpoint\")\n",
    "    print(\"   This is fine for learning and testing!\")\n",
    "    print(\"\\nüí° Make sure to use: from application_endpoint_mock import ApplicationEndpoint\")\n",
    "    print(\"\\nüìù To use a real endpoint later, add to .credentials.env:\")\n",
    "    print(\"   APPLICATION_ENDPOINT=\\\"https://your-app.azurewebsites.net/score\\\"\")\n",
    "    print(\"   APPLICATION_KEY=\\\"your-api-key\\\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Application Endpoint Options\n",
    "\n",
    "You have three options for the application endpoint:\n",
    "\n",
    "#### Option 1: Use Mock Endpoint (Recommended for Learning)\n",
    "If you don't have access to the shared endpoint, use the mock version:\n",
    "- Change the import below from `application_endpoint` to `application_endpoint_mock`\n",
    "- No need for APPLICATION_ENDPOINT or APPLICATION_KEY in `.credentials.env`\n",
    "- The mock simulates responses about Microsoft Responsible AI\n",
    "\n",
    "#### Option 2: Use Shared Endpoint (If Available)\n",
    "Add to `.credentials.env`:\n",
    "```bash\n",
    "APPLICATION_ENDPOINT=\"https://your-app-endpoint.azurewebsites.net/score\"\n",
    "APPLICATION_KEY=\"your-application-api-key\"\n",
    "```\n",
    "\n",
    "#### Option 3: Deploy Your Own RAG Application\n",
    "See `DEPLOYMENT_GUIDE.md` for instructions on deploying your own RAG endpoint using Azure AI Foundry.\n",
    "\n",
    "**Current configuration:** Check the import statement in the evaluation cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "EvaluationException",
     "evalue": "(UserError) Failed to upload evaluation run to the cloud due to insufficient permission to access the storage. Please ensure that the necessary access rights are granted.\nVisit https://aka.ms/azsdk/python/evaluation/remotetracking/troubleshoot to troubleshoot this issue.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_eval_run.py:447\u001b[0m, in \u001b[0;36mEvalRun.log_artifact\u001b[0;34m(self, artifact_folder, artifact_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(local, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m--> 447\u001b[0m             blob_client\u001b[38;5;241m.\u001b[39mupload_blob(fp, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HttpResponseError \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/azure/core/tracing/decorator.py:119\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/azure/storage/blob/_blob_client.py:631\u001b[0m, in \u001b[0;36mBlobClient.upload_blob\u001b[0;34m(self, data, blob_type, length, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blob_type \u001b[38;5;241m==\u001b[39m BlobType\u001b[38;5;241m.\u001b[39mBlockBlob:\n\u001b[0;32m--> 631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m upload_block_blob(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blob_type \u001b[38;5;241m==\u001b[39m BlobType\u001b[38;5;241m.\u001b[39mPageBlob:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/azure/storage/blob/_upload_helpers.py:197\u001b[0m, in \u001b[0;36mupload_block_blob\u001b[0;34m(client, stream, overwrite, encryption_options, blob_settings, headers, validate_content, max_concurrency, length, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     process_storage_error(error)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ResourceModifiedError \u001b[38;5;28;01mas\u001b[39;00m mod_error:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/azure/storage/blob/_shared/response_handlers.py:195\u001b[0m, in \u001b[0;36mprocess_storage_error\u001b[0;34m(storage_error)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# `from None` prevents us from double printing the exception (suppresses generated layer error context)\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     exec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise error from None\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=exec-used # nosec\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: Key based authentication is not permitted on this storage account.\nRequestId:3dc5313a-a01e-0024-1981-428d59000000\nTime:2025-10-21T11:53:37.8635996Z\nErrorCode:KeyBasedAuthenticationNotPermitted\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>KeyBasedAuthenticationNotPermitted</Code><Message>Key based authentication is not permitted on this storage account.\nRequestId:3dc5313a-a01e-0024-1981-428d59000000\nTime:2025-10-21T11:53:37.8635996Z</Message></Error>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEvaluationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m current_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m evaluation_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mManual-Data-Eval-Run-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     42\u001b[0m     evaluation_name\u001b[38;5;241m=\u001b[39mevaluation_name,\n\u001b[1;32m     43\u001b[0m     data\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m     44\u001b[0m     target\u001b[38;5;241m=\u001b[39mApplicationEndpoint(),\n\u001b[1;32m     45\u001b[0m     evaluators\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_safety\u001b[39m\u001b[38;5;124m\"\u001b[39m: content_safety_evaluator,\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoherence\u001b[39m\u001b[38;5;124m\"\u001b[39m: coherence_evaluator,\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevance\u001b[39m\u001b[38;5;124m\"\u001b[39m: relevance_evaluator,\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundedness\u001b[39m\u001b[38;5;124m\"\u001b[39m: groundedness_evaluator,\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfluency\u001b[39m\u001b[38;5;124m\"\u001b[39m: fluency_evaluator,\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m: similarity_evaluator,\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundedness_pro\u001b[39m\u001b[38;5;124m\"\u001b[39m: groundedness_pro_eval,\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindirect_attack\u001b[39m\u001b[38;5;124m\"\u001b[39m: indirect_attack_evaluator,\n\u001b[1;32m     54\u001b[0m     },\n\u001b[1;32m     55\u001b[0m     azure_ai_project\u001b[38;5;241m=\u001b[39mazure_ai_project,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Add credential for storage access\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     credential\u001b[38;5;241m=\u001b[39mDefaultAzureCredential(),\n\u001b[1;32m     58\u001b[0m     evaluator_config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_safety\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{target.response}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}},\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoherence\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{target.response}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}},\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevance\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{target.response}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.context}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     63\u001b[0m         },\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundedness\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     66\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     67\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{target.response}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.context}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m             }\n\u001b[1;32m     70\u001b[0m         },\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundedness_pro\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{target.response}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.context}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m             }\n\u001b[1;32m     77\u001b[0m         },\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindirect_attack\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     79\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     80\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{target.response}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m             }\n\u001b[1;32m     83\u001b[0m         },\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfluency\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{target.response}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     86\u001b[0m         },\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{target.response}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{data.ground_truth}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     89\u001b[0m         },\n\u001b[1;32m     90\u001b[0m     },\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:839\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, tags, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EvaluationException):\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[1;32m    833\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    834\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mEVALUATE,\n\u001b[1;32m    835\u001b[0m         category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mFAILED_EXECUTION,\n\u001b[1;32m    836\u001b[0m         blame\u001b[38;5;241m=\u001b[39mErrorBlame\u001b[38;5;241m.\u001b[39mSYSTEM_ERROR,\n\u001b[1;32m    837\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:796\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, tags, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m     user_agent: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_agent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m UserAgentSingleton()\u001b[38;5;241m.\u001b[39madd_useragent_product(user_agent) \u001b[38;5;28;01mif\u001b[39;00m user_agent \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext():\n\u001b[0;32m--> 796\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate(\n\u001b[1;32m    797\u001b[0m             evaluation_name\u001b[38;5;241m=\u001b[39mevaluation_name,\n\u001b[1;32m    798\u001b[0m             target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[1;32m    799\u001b[0m             data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    800\u001b[0m             evaluators_and_graders\u001b[38;5;241m=\u001b[39mevaluators,\n\u001b[1;32m    801\u001b[0m             evaluator_config\u001b[38;5;241m=\u001b[39mevaluator_config,\n\u001b[1;32m    802\u001b[0m             azure_ai_project\u001b[38;5;241m=\u001b[39mazure_ai_project,\n\u001b[1;32m    803\u001b[0m             output_path\u001b[38;5;241m=\u001b[39moutput_path,\n\u001b[1;32m    804\u001b[0m             fail_on_evaluator_errors\u001b[38;5;241m=\u001b[39mfail_on_evaluator_errors,\n\u001b[1;32m    805\u001b[0m             tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    806\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    807\u001b[0m         )\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;66;03m# Handle multiprocess bootstrap error\u001b[39;00m\n\u001b[1;32m    810\u001b[0m     bootstrap_error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn attempt has been made to start a new process before the\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent process has finished its bootstrapping phase.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:978\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(evaluators_and_graders, evaluation_name, target, data, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, tags, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m     studio_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trace_destination:\n\u001b[0;32m--> 978\u001b[0m         studio_url \u001b[38;5;241m=\u001b[39m _log_metrics_and_instance_results(\n\u001b[1;32m    979\u001b[0m             metrics, results_df, trace_destination, \u001b[38;5;28;01mNone\u001b[39;00m, evaluation_name, name_map, tags\u001b[38;5;241m=\u001b[39mtags, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    980\u001b[0m         )\n\u001b[1;32m    982\u001b[0m result_df_dict \u001b[38;5;241m=\u001b[39m results_df\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    983\u001b[0m result: EvaluationResult \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m: result_df_dict, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudio_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: studio_url}  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_utils.py:269\u001b[0m, in \u001b[0;36m_log_metrics_and_instance_results\u001b[0;34m(metrics, instance_results, trace_destination, run, evaluation_name, name_map, tags, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tmp_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mDefaultOpenEncoding\u001b[38;5;241m.\u001b[39mWRITE) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    267\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(instance_results\u001b[38;5;241m.\u001b[39mto_json(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m--> 269\u001b[0m ev_run\u001b[38;5;241m.\u001b[39mlog_artifact(tmpdir, artifact_name)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Using mlflow to create a dummy run since once created via PF show traces of dummy run in UI.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Those traces can be confusing.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# adding these properties to avoid showing traces if a dummy run is created.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# We are doing that only for the pure evaluation runs.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_eval_run.py:454\u001b[0m, in \u001b[0;36mEvalRun.log_artifact\u001b[0;34m(self, artifact_folder, artifact_name)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    450\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    451\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to upload evaluation run to the cloud due to insufficient permission to access the storage.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please ensure that the necessary access rights are granted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         )\n\u001b[0;32m--> 454\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[1;32m    455\u001b[0m             message\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m    456\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mEVAL_RUN,\n\u001b[1;32m    457\u001b[0m             category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mFAILED_REMOTE_TRACKING,\n\u001b[1;32m    458\u001b[0m             blame\u001b[38;5;241m=\u001b[39mErrorBlame\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    459\u001b[0m             tsg_link\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aka.ms/azsdk/python/evaluation/remotetracking/troubleshoot\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    460\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# To show artifact in UI we will need to register it. If it is a promptflow run,\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# we are rewriting already registered artifact and need to skip this step.\u001b[39;00m\n",
      "\u001b[0;31mEvaluationException\u001b[0m: (UserError) Failed to upload evaluation run to the cloud due to insufficient permission to access the storage. Please ensure that the necessary access rights are granted.\nVisit https://aka.ms/azsdk/python/evaluation/remotetracking/troubleshoot to troubleshoot this issue."
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    "    GroundednessProEvaluator,\n",
    "    IndirectAttackEvaluator,\n",
    ")\n",
    "\n",
    "# OPTION 1 (MOCK): Use this if you don't have a real endpoint\n",
    "from application_endpoint_mock import ApplicationEndpoint\n",
    "\n",
    "# OPTION 2 (REAL): Comment out the line above and uncomment this if you have a real endpoint\n",
    "# from application_endpoint import ApplicationEndpoint\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "content_safety_evaluator = ContentSafetyEvaluator(\n",
    "    azure_ai_project=azure_ai_project, credential=DefaultAzureCredential()\n",
    ")\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "groundedness_pro_eval = GroundednessProEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "indirect_attack_evaluator = IndirectAttackEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "\n",
    "path = str(pathlib.Path(pathlib.Path.cwd())) + \"/manual_data.jsonl\"\n",
    "\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "evaluation_name = f\"Manual-Data-Eval-Run-{current_date}\"\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=evaluation_name,\n",
    "    data=path,\n",
    "    target=ApplicationEndpoint(),\n",
    "    evaluators={\n",
    "        \"content_safety\": content_safety_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "        \"similarity\": similarity_evaluator,\n",
    "        \"groundedness_pro\": groundedness_pro_eval,\n",
    "        \"indirect_attack\": indirect_attack_evaluator,\n",
    "    },\n",
    "    # azure_ai_project=azure_ai_project,\n",
    "    # Add credential for storage access\n",
    "    credential=DefaultAzureCredential(),\n",
    "    evaluator_config={\n",
    "        \"content_safety\": {\"column_mapping\": {\"query\": \"${data.query}\", \"response\": \"${target.response}\"}},\n",
    "        \"coherence\": {\"column_mapping\": {\"query\": \"${data.query}\", \"response\": \"${target.response}\"}},\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\"query\": \"${data.query}\", \"response\": \"${target.response}\", \"context\": \"${data.context}\"}\n",
    "        },\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${target.response}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "            }\n",
    "        },\n",
    "        \"groundedness_pro\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${target.response}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "            }\n",
    "        },\n",
    "        \"indirect_attack\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${target.response}\",\n",
    "            }\n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"column_mapping\": {\"query\": \"${data.query}\", \"response\": \"${target.response}\"}\n",
    "        },\n",
    "        \"similarity\": {\n",
    "            \"column_mapping\": {\"response\": \"${target.response}\", \"ground_truth\": \"${data.ground_truth}\"}\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results as output of the Evaluate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results[\"rows\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view detail result in AI Project for quality also risk and safety environments.\n",
    "<img width=\"60%\" height=\"60%\" src=\"aifoundryevaltab.png\" alt=\"Ai Foundry SDK\" />\n",
    "\n",
    "<img width=\"60%\" height=\"60%\" src=\"evalrundetail.png\" alt=\"Ai Foundry SDK\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
